<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Talks and Seminars | Bryan Lawrence</title>
  <meta name="description" content="Intermittent rambings on climate, computing, or data science, what I'm up to, and what I'm thinking. Mostly professional, sometimes not.
">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://0.0.0.0:4000/talks/">
  <link rel="alternate" type="application/rss+xml" title="Bryan Lawrence" href="http://0.0.0.0:4000/feed.xml" />
<link rel='stylesheet' id='open-sans-css'  href='//fonts.googleapis.com/css?family=Open+Sans%3A300italic%2C400italic%2C600italic%2C300%2C400%2C600&#038;subset=latin%2Clatin-ext&#038;ver=4.2.4' type='text/css' media='all' />
<link href='http://fonts.googleapis.com/css?family=Titillium+Web:600italic,600,400,400italic' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="https://www.osu.edu/assets/fonts/webfonts.css">
<link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

<link rel="author" href="https://plus.google.com/u/0/110288225974212014913"/>
<meta name="google-site-verification" content="nfd8hkZSc3GmIivrLCudAdTq5m1kI0Ao7P1UyIt9z5Q" />



<!-- Twitter cards -->
<meta name="twitter:site"    content="@bnlawrence">
<meta name="twitter:title"   content="Talks and Seminars">


<meta name="twitter:description" content="">



<meta name="twitter:card"  content="summary">
<meta name="twitter:image" content="">

<!-- end of Twitter cards -->

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Bryan Lawrence</a>


    <nav class="site-nav">

      <a href="#" class="menu-icon menu.open">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

    <div class="trigger"><h1>Main Navigation</h1>

 <ul class="menu">

    
    
     <li><a href="/" class="page-link">Home</a>
    
    </li>
    
    
     <li><a href="/about/" class="page-link">About</a>
    
    </li>
    
    
    <li><a href="/academic/" class="page-link">Academic</a>
    <ul class="sub-menu">
    
    <li><a href="/academic/">Academic</a></li>
    
    <li><a href="/projects/">Projects</a></li>
    
    <li><a href="/publications/">Publications</a></li>
    
    <li><a href="/cv/">Short CV</a></li>
    
    <li><a href="/research/">Research</a></li>
    
    <li><a href="/talks/">Talks</a></li>
    
    </ul>
    
    </li>
    
    
     <li><a href="/blog/" class="page-link">Blog</a>
    
    </li>
    
    
     <li><a href="/search/" class="page-link">Find</a>
    
    </li>
    
    </ul>


     </div>
    </nav>
</div>
<div class="wrapper">
   <p> Intermittent rambings on climate, computing, or data science, what I'm up to, and what I'm thinking. Mostly professional, sometimes not.
 </p>
</div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Talks and Seminars</h1>
  </header>

  <article class="post-content">
    <p>Since 2006 I’ve been recording my significant talks here (see also the <a href="/talks/summary">summary list</a>).</p>

<div class="talks">

<div class="meeting">
<h1 class="theader"> UK Turbulence Consortium, 2023 Annual Meeting</h1>
<h1 class="tdetails">Imperial College, London, March 26, 2023</h1>
<p>Presentation: <a href="/assets/talks/2023-03-27-data_for_uktc.pdf">pdf</a> (13 MB)</p>

<p>This is a talk in three parts, covering some motivation for investing in data management, some new developments underway to deal with high volume data, and a reminder of the importance of FAIR.</p>

<p>Part one aims to introduce other scientific communities to why and how climate science uses data management in delivering model intercomparison. Using examples from how the IPCC process uses CMIP data, there is a walk through some of the technology underpinning CMIP6 (the CF conventions, data analysis stacks, ESGF etc), and a description of some of the (especially European) data delivery. There is a good deal of motivation for why we do model intercomparison and how it both delivers and supports projections using scenarios. Examples of how high level software stacks remove labour from scientists are shown.</p>

<p>Part two describes some of the work being done in our <a href="https://excalibur.ac.uk/">Excalibur</a> cross-cutting data projects, targeting those pieces most relevant to meeting attendees: Standards based Aggregation, Active Storage, and support for in-flight ensemble analysis.</p>

<p>Part three concludes the talk with a short discussion of how FAIR applies to simulation data.</p>

</div>

<div class="meeting">
<h1 class="theader"> ESiWACE2 2021 General Assembly</h1>
<h1 class="tdetails">Gotomeeting, September 27, 2021</h1>
<p>Presentation: <a href="/assets/talks/2021-09-27-esiwace2-ga-wp4.pdf">pdf</a> (4 MB)</p>

<p>An update on progress in the ESiWACE2 fourth work package, WP4, data (systems) at scale.</p>

<p>We discussed the WP4 objectives and our progress on three different pieces of technology: support for in-flight ensemble diagnostics, semantic storage tools, and the Earth System Data Middleware (ESDM). I gave a description of the first two, and <a href="https://hps.vi4io.org/about/people/julian_kunkel">Julian</a> described the ESDM and accompanying activities.</p>

<p>For my bits:</p>
<ul>
  <li>We showed some great results from our use of XIOS for ensembles, with scaling results from a one-hundred member N96 system and sixteen-member N512 system. We’re convinced that when we do this for real for high-resolution models the extra work to do the ensemble diagnostics will be lost in the noise. We have more work to do to do it for real though, including setting up real diagnostics for each member, and the new ensemble diagnostics. We’ve got a cool new CYLC system to handle an ensemble member crashing during the run.</li>
  <li>We also showed our plans for moving forward with semantic storage technologies as part of both the rest of ESiWACE2 and ExCALIBUR. There will be work on a new tape and object store interface for deployment on HPC platforms remote from the storage and analysis platform (e.g. for use on ARCHER2 writing to JASMIN), and on our new aggregation syntax (of which much more in the future).</li>
</ul>

</div>

<div class="meeting">
<h1 class="theader"> 19th Workshop on high performance computing in meteorology</h1>
<h1 class="tdetails">Zoom, September 20, 2021</h1>
<p>Presentation: <a href="/assets/talks/2021-09-20_digital_twins.pdf">pdf</a> (0.3MB)</p>

<p>Digital twins are defined in many ways, but definitions usually have some combination of the words “simulation of the real world constrained by real time data”. These definitions basically encompass our understanding of the use of weather and climate models whether using data assimilation or tuning to history. It would appear that we (environmental science in general and weather and climate specifically) have been doing digital twins for a long time. So what then should new digital twin projects in our field aim to do that is different from our business as usual approach of evolving models and increasing use of data?</p>

<p>There seem to be two key promises associated with the current drive towards digital twinning: scenario evaluation and democratising access to simulations. To make progress we can consider two specific exemplar questions: (1) How can I better use higher-resolution local models of phenomenon X driven by lower-resolution (yet still expensive large-scale) global models?; and (2) How can I couple my socio-economic model into this system?</p>

<p>Delivering on these promises, even for just these two exemplars, requires some additional thinking beyond what we do now, and such thinking can take advantage of our changing computing environments, and in particular our use of tiered memory and storage, so that the burden of supporting these digital twin applications does not come with unaffordable complexity or cost in the primary simulation environment. It will also require much more transparency and community engagement in the experimental definition. In this talk I outline some of the possibilities in this space, building from existing technologies (software and hardware), models, and infrastructure (existing systems), and suggest some practical ways forward for delivering on some of the digital twin promises, some of which might even be possible within existing programmes such as the EC’s Destination Earth (DestinE).</p>

<p>This talk was part of the <a href="https://events.ecmwf.int/event/169/timetable/">19th Workshop on high performance computing in meteorology</a>, and there is a <a href="https://events.ecmwf.int/event/169/contributions/2723/attachments/1423/2564/go">recording available</a> (not that I recommend listening all the way to the questions which I didn’t handle very well; I was rather tired when I gave this talk).</p>

</div>

<div class="meeting">
<h1 class="theader"> ESCAPE-2 Final Dissemination Workshop</h1>
<h1 class="tdetails">Zoom, September 03, 2021</h1>
<h2 id="excalibur-for-escape">ExCALIBUR for ESCAPE</h2>

<p>Presentation: <a href="/assets/talks/2021-09-03-ExCALIBUR-for-ESCAPE.pdf">pdf</a> (1.7MB)</p>

<p>The Exascale Computing ALgorithms &amp; Infrastructures for the Benefit of UK Research (<a href="https://excalibur.ac.uk">ExCALIBUR</a>) programme is a five year activity with two delivery partners: the <a href="https://www.metoffice.gov.uk">Met Office</a>(for PSREs amd the <a href="https://ukri.org/councils/nerc">NERC</a> community) and the Engineering and Physical Sciences Research Council (<a href="httpsL//ukri.org/councils/epsrc">EPSRC</a>). The aim of the project is to redesign high priority simulation codes and algorithms to fully harness the power of future supercomputers, keeping UK research and development at the forefront of high-performance simulation science.</p>

<p>This talk (part of the <a href="https://www.hpc-escape2.eu/outreach/events/escape-2-final-dissemination-workshop">ESCAPE-2 Final Dissemination Workshop</a>) outlines key aspects of the programme, including a summary of the priority use cases with a bit more detail about the environmental science projects (which are all related to the next generation modelling system which will replace the current Unified Model). Some cross-cutting areas are listed and there is a summary of our ExCALIData project (see <a href="http://0.0.0.0:4000/talks/2021/09/esi-to-exc/">my last talk</a>).</p>

</div>

<div class="meeting">
<h1 class="theader"> Benchmarking for ExCALIBUR Update</h1>
<h1 class="tdetails">Zoom, September 02, 2021</h1>
<h2 id="from-esiwace-to-excalidata">From ESiWACE to ExCALIData</h2>

<p>I gave a talk to the ExCALIBUR <a href="https://excalibur.ac.uk/themes/hardware-and-enabling-software/">benchmarking community</a> about the work we plan to do in the <a href="https://excalibur.ac.uk">ExCALIBUR</a> programme.   This new activity is called ExCALIData, and really comprises two projects ExCALIStore and ExCALIWork. It is “cross-cutting” work funded as part of the <a href="https://www.metoffice.gov.uk">Met Office</a> strand of ExCALIBUR to deliver cross-cutting research in support of all programme elements (see the presentation in my <a href="http://0.0.0.0:4000/talks/2021/09/excalibur4escape/">next post</a> for details). The entire programme is a national programme with two funding strands, one for public sector research establishments <em>and</em> the <a href="https://ukri.org/councils/nerc">NERC</a> environmental science research community, and one delivered by <a href="https://ukri.org/councils/epsrc">EPSRC</a> for the rest of the <a href="ukri.org">UKRI</a> community.</p>

<p>Excalibur itself is a multi-million pound multi-year project which aims to deliver software suitable for the next generation of supercomputing at scale - the “exascale” generation. The two ExCALIData projects have been funded in response to a call which essentially asked for two projects which addressed:</p>

<ul>
  <li>support for optimal (performant) use of a given system’s storage configuration (portable) for their particular application without having to know the details of the system configuration (productive) in order to configure their application; and</li>
  <li>deliver a completely new paradigm for where certain computations are performed by reducing the amount of data movement needed, particularly in the context of ensembles (but implicitly, make sure that this can be done productively, and results in portable code which is optimally performant in any given environment).</li>
</ul>

<p>The call asked for particular application to any one of the existing Excalibur use cases but a plan for how it might be applicable to others. We of course addressed climate modelling as our first use-case, but in doing so aimed to address a couple of other use cases and build some generic tools.</p>

<p>We were in a good place to bid for this work building on the back of our <a href="https://esiwace.eu">EsIWACE</a> activities on I/O and workflow, so the talk I gave to the benchmarking community was in two halves: the first half addressed the problem statement and some of the things we have been doing in ESiWACE, and the second described the programme of work we plan in Excalibur.</p>

<h4 id="from-esiwace-to-excalibur">From EsiWACE to ExCALIBUR</h4>

<p>Presentation: <a href="/assets/talks/2021-09-02_exa_io.pdf">pdf</a>  (1.6MB)</p>

<p>This was a short motivation for our particular problem in terms of massive grids of data from models and how increasing resolution leads (if nothing is done) to vastly more data to store and analyse. The trends in data volumes match the trends in computing and will lead to exabytes of simulation data before we hit exaflops.  Our exabytes are a bit more difficult to deal with (in some ways) than Facebook’s, but like all the big data specialists, as a community we are building our own customised computing environments - in this case we are specialising the analysis environment sooner than we have specialised the simulation platform (but that too is coming).</p>

<p>In WP4 of EsiWACE(2) we have been working to “mitigate the effects of the data deluge from high-resolution simulations”, by working on tools for carrying out ensemble statistics “in-flight” and on tools to hide storage complexity and deliver portable workflows. There were several components to that work: the largest of which has been the development of the Earth System Data Middleware (ESDM), an activity led by <a href="https://hps.vi4io.org/about/people/julian_kunkel">Prof Julian Kunkel</a>. The others were mainly led by me, and are the main thrust of the ExCALIData work, although we will be doing some comparisons of other technologies with the ESDM.</p>

<h4 id="an-introduction-to-excalidata">An Introduction to ExCALIData</h4>

<p>Presentation: <a href="/assets/talks/2021-09-02-ExCALIData.pdf">pdf</a> (1.6 MB)</p>

<p>ExCALIData aims to address the end-to-end workflow for (large)data in, simulation, (larger)data out, analysis, store workflows. We designed two complementary projects to address the two goals in the call with six main activities (three to each project) and one joint work package on knowledge-exchange.  (They were designed in such a way that only one could have been funded, but we could get synergy from having two.)</p>

<p>The six work packages address</p>

<ol>
  <li>storage interfaces, and our our idea of semantically interesting “atomic_datasets” as an interface to multiple constituent data elements (quarks) distributed across storage media.</li>
  <li>how best we can use fabric and solid state storage. There are a plethora of technology options, but how can we deploy some of them for real? Which ones?</li>
  <li>A comparison of I/O middleware (including the domain specific ESDM, and the generic ADIOS2)</li>
  <li>Active Storage in software, and</li>
  <li>Active Storage in hardware</li>
  <li>Extending I/O server functionality that we developed in ESiWACE for “atmosphere only” workflows for coupled model workflows.</li>
</ol>

<p>For the active storage work the key idea is to deploy something actually usable by scientists and build two demonstrator storage systems which deliver the active storage functionality.</p>

<p>Not surprisingly, this is a complex project, and we needed a lot of partners, in this case: DDN and StackHPC to help us with the storage servers and the University of Cambridge to help us with some of the more technical hardware activities (especially the work on fabric, solid state, storage, and the benchmarking of comparative solutions).</p>


</div>

<div class="meeting">
<h1 class="theader"> ESIWACE2 Mid-Term Review</h1>
<h1 class="tdetails">GotoMeeting, October, 2020</h1>
<h2 id="data-systems-at-scale">Data Systems at Scale</h2>

<p>Presentation: <a href="/assets/talks/201021_esiwace2_wp4.pdf">pdf</a> (0.5MB)</p>

<p>This was a presentation given as part of the <a href="https://www.esiwace.eu/">esiwace</a> mid-term review. It describes some of the progress made in one of the work packages,  WP4, which is about data systems at scale. Here the definition of “at scale” means “for weather and climate simulation on exascale (and pre-exascale) supercomputing” <em>and</em> for downstream analysis systems.</p>

<p>I have an <a href="/talks/2019/03/Data-Systems/">older post</a> linking to a talk describing the work package objectives.</p>

<p>In this presentation I highlighted</p>
<ul>
  <li>our work on ensemble handling. Amongst other activities we have demonstrated “in-flight ensemble data processing” with a large high resolution ensemble (10 member global 25 km resolution using 50K cores).</li>
  <li>progress with the Earth System Data Middleware (<a href="https://github.com/ESiWACE/esdm">ESDM</a>), which includes NetCDF interface and some new backends, and</li>
  <li>work by our industry colleagues, Seagate and DDN both on ESDM backends and on new active storage systems which will be able to do simple manipulations “in storage”,  and</li>
  <li>our work on <a href="https://github.com/cedadev/S3-netcdf-python">S3NetCDF</a>, a drop in extension for netcdf-python suitable for use with object stores.</li>
</ul>


</div>

<div class="meeting">
<h1 class="theader"> Earthcube - What about Data?</h1>
<h1 class="tdetails">Boulder via Hangout, May 2020</h1>
<h2 id="when-and-how-should-a-simulation-be-fair">When (and how) should a simulation be Fair?</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2020-05-05-fair.pdf">pdf</a> (4 MB).</p>

<p>After a reminder about FAIR (Findable, Accessible, Interoperable, Reusable - not Reproducible), I discuss some of the issues about applying these concepts to Simulation Data. Apart from the volume issues, we have to decide just what “Simulation Data” actually means, does it mean the entire workflow, or just the outputs.  There are a huge variety of types of simulation, are they all important? Using the ES-DOC vocabularies, I point out that in practice simulations are not meaningfully reproducible - except in trivial cases, and actually it is more important to reproduce experiments - which is of course the heart of model intercomparison. At <a href="https://ceda.ac.uk">CEDA</a> we have two decades of experience curating data, and we now use the JASMIN platform to provide a data commons to make data accessible. Lots of simulation data is analysed on JASMIN, alongside the CEDA archive, but for fifteen years we have made decisions about whether to archive simulation data based on (what was then) the BADC Numerical Model Data Policy.  The key insight in developing that policy was that it’s relatively easy to decide what’s important, and what’s not important, but there is a lot of middle ground for which value judgements are important. We developed polices to decide on the important and not important data, and to guide the value judgements, and these are summarised here. Underlying all decisions are questions about affordabilty, and whether or not adequate metadata can be (or will be) produced. Without such metadata curation of simulation data becomes pointless. The bottom line is that not all simulation data should be FAIR, but that which needs to be FAIR needs to be well documented. Money matters as simulation data at scale is incredibly expensive.</p>

</div>

<div class="meeting">
<h1 class="theader"> The UM User's Workshop 2019 - Next Generation Modellin Systems</h1>
<h1 class="tdetails">Exeter, June, 2019</h1>
<h2 id="challenges-facing-the-modelling-community">Challenges facing the modelling community</h2>

<p>Presentation: <a href="/assets/talks/2019-06-17-lawrence-umuw19.pdf">pdf</a> (8 MB)</p>

<p>This was a talk given to set the scene for the Next Generation Modelling System sessions of the 2019 Unified Model (Um) 
Users workshop - a meeting of the international UM partnership bringing together indivudals from the US, Korea, Australia, 
NZ, and the UK.</p>

<p>It was a shortened version of my <a href="/talks/2019/03/end-of-climate-modelling/">seminar</a> on the end of climate modelling, where
I was aiming to explain to some of the attendees why the radical changes in model structure coming with our next 
generation LFRic model are necessary given some of the technologies in play. I was able to omit some of the key parts of
the seminar because there were other speakers addressing those subjects (e.g. Rupert Ford was up later talking about
Domain Specific Languages).</p>


</div>

<div class="meeting">
<h1 class="theader"> ESIWACE2 Kick-Off</h1>
<h1 class="tdetails">Hamburg, March, 2019</h1>
<h2 id="data-systems-at-scale">Data Systems at Scale</h2>

<p>Presentation: <a href="/assets/talks/190312_esiwace2_wp4.pdf">pdf</a> (0.5MB)</p>

<p>This was a presentation given as part of the <a href="https://www.esiwace.eu/events/joint-meeting-2019/esiwace-annual-meeting-and-esiwace2-kickoff-meeting">esiwace2 kickoff</a>. It describes one of the work packages,  WP4, which is about data systems at scale. Here the definition of “at scale” means “for weather and climate simulation on exascale (and pre-exascale) supercomputing” <em>and</em> for downstream analysis systems. We expect I/O performance and data volumes will limit their exploitation without new storage and analysis libraries.</p>

<p>This work package describes an ambitious programme of work, building on lessons learnt from ESiWACE. It will address a range of issues in workflow, from modifications to a workflow engine (<a href="https://cylc.github.io/">cylc</a>) and a scheduler (<a href="https://slurm.schedmd.com/">slurm</a>), to new I/O libraries and data handling tools. Some integration with <a href="https://pangeo.io/">pangeo</a> is likely. There is a key role for ensemble analysis tools to mitigate aqainst writing data in the first place.</p>

<p>A key goal will be to deploy these new tool in real workflows, within the project, and elsewhere, and to develop them in a way that has a credible path to sustainability. In practice deployability is likely to involve three key facets:</p>

<ul>
  <li>portability, to ensure a wider range of stakeholders,</li>
  <li>containerisation, to deliver as much as possible within userspace, and</li>
  <li>vendor engagement alongside open-source products, allowing differentiation in the market and realistic business models for those developing customised high-performance storage solutions.</li>
</ul>

<p>The work package is being led from the University of Reading (Computer Science, and NCAS), with partners from CNRS-IPSL, STFC, Seagate, DDN, DKRZ, ICHEC, and the Met Office.</p>

<p>(See <a href="/talks/2020/10/Data-Systems/">mid-term update</a> for an update on progress.)</p>


</div>

<div class="meeting">
<h1 class="theader"> NCAS Seminar</h1>
<h1 class="tdetails">Reading, March, 2019</h1>
<h2 id="the-end-of-climate-modelling-as-we-know-it">The end of Climate Modelling as we know it</h2>

<p>Presentation: <a href="/assets/talks/2019-03-01-uor-ncas-seminar.pdf">pdf</a> (15 MB)</p>

<p>This was a seminar given in Meteorology at the University of Reading as part of the NCAS seminar series.</p>

<p>In this presentation I was aiming to introduce a meteorology and climate science community to the upcoming difficulties with computing: from the massive explosion in hardware variety that we associate with the end of Moore’s Law, to the fact that in about a decade we may only be able to go faster by spending more money on power (provided we have the parallelisation). Similar problems with storage arising from both costs and bandwidth are coming …</p>

<p>I set this in the scene of our existing experience of decades of unbridled increases in computing at more or less the same power consumption until the last decade when we’ve still had more compute, but at the same power cost per flop (for CPUs at least).</p>

<p>Of course it wont mean the end of climate science, and it will probably take longer to arrive than currently being predicted, but it does mean a change in the way we organise ourselves, in terms of sharing data and experiment design, and it certainly means “going faster” will require being smarter, with maths, AI etc.</p>

<p>This seminar differs from my <a href="/talks/2019/02/supercomputing-jasmin-diversity/">last one</a> in that I didn’t talk about JASMIN at all, and I spent more time on the techniques we will need to <a href="https://doi.org/10.5194/gmd-11-1799-2018">cross the chasm</a> between science code and the changing hardware.</p>

<p>I finished with some guidance for our community: we will need to become better at using the right tool(s) for the job, rather than treating everything as a nail because we have a really cool hammer:</p>

<ul>
  <li>More use of hierarchy of models;</li>
  <li>Precision use of resolution;</li>
  <li>Selective use of complexity;</li>
  <li>Less use of “ensembles of opportunity”, much more use of “designed ensembles”;</li>
  <li>Duration and Ensemble Size : thinking harder a priori about what is needed;</li>
  <li>Much more use of emulation.</li>
</ul>

</div>

<div class="meeting">
<h1 class="theader"> Computer Science Seminar</h1>
<h1 class="tdetails">Reading, February, 2019</h1>
<h2 id="supercomputers-are-no-longer-all-the-same-and-it-will-get-worse-a-climate-modelling-perspective">Supercomputers are no longer all the same and it will get worse; a climate modelling perspective</h2>

<p>Presentation: <a href="/assets/talks/2019-02-11-uor-cosc-seminar.pdf">pdf</a> (30 MB)</p>

<p>This was a seminar given in Computer Science at the University of Reading. I was aiming to kick the ball through a few balls at once, being introductions to</p>

<ul>
  <li>Climate modelling for computer scientists, to get across the size of the computational and data handling problems, leading to</li>
  <li>The changing way we deal with simulation data - from “download” to “remote access”, and the need for</li>
  <li>New data analysis supercomputers, and in particular, JASMIN.</li>
</ul>

<p>From there I switched gear to</p>

<ul>
  <li>A discussion of Moore’s Law, and it’s demise, and</li>
  <li>How we can still make progress in (climate) science with new maths (including AI and ML), new algorithms, new programming techniques, and new ways of working with data.</li>
</ul>

<p>The bottom line I wanted to get across to this audience was that there is lots of scope for computer science, and lots of interesting things to do.</p>


</div>

<div class="meeting">
<h1 class="theader"> NERC HPC Strategy</h1>
<h1 class="tdetails">London, November, 2018</h1>
<h2 id="trends-and-context-for-environmental-hpc">Trends and Context for Environmental HPC</h2>

<p>Presentation: <a href="/assets/talks/181127_nerc_hpc_bnl_v1.pdf">pdf</a> (7 MB)</p>

<p>This was the kickoff talk to a NERC “town hall meeting” workshop envisaged as part of the establishment of a formal NERC HPC strategy. After my talk, we had a three component workshop: some international talks, some breakout discussions, and some local talks. The main aim of the meeting was</p>

<blockquote>
  <p>To begin a process of establishing and maintaining a NERC strategy for high
performance computing which reflects the scientific goals of the UK environmental science
community, government objectives, and the need for international competitiveness.</p>
</blockquote>

<p>The breakout groups covered four themes:</p>
<ol>
  <li>How does our science face a future where computers do not run faster? Are there known algorithmic
routes to improve time to solution for fixed FLOPs (but maybe enhanced parallelism) that we are not
yet exploring? What else can we do?</li>
  <li>Which HPC related international collaborations do we depend upon, and how can we sustain these,
intellectually and financially?</li>
  <li>What are the implications of the new UKRI funding routes (e.g. Industrial Strategy Challenge Fund)
for environmental HPC resource requirements (software, hardware, people)?</li>
  <li>How do we gather information about the impact of our HPC related research? What are the routes
to demonstrating economic impact?</li>
</ol>

<p>… and my talk was attempting to provide an introduction and context for these objectives and breakouts.  The key point of course is that free lunch of easy (and relatively cheap) computing is over. Advancing along our traditional axes of resolution, complexity, ensembles etc, will be complicated by harder to use and (relatively) more expensive computing and storage.</p>

<p>I finished with six key points:</p>
<ul>
  <li>The NERC HPC recurrent budget is currently fixed, but there is usage growth via both growth within the existing communities, and the addition of new communities.</li>
  <li>Future computing will not be any faster than current computing, we will only get to do-more/go-faster by expending energy (electricy) and by being smarter about using parallelism, by algorithms, or software, or both.</li>
  <li>HPC platforms are changing, with more heterogeneity and customisation, and that’s happening just as we have to take on machine learning as a new tool.</li>
  <li>NERC needs plans and evidence to justify HPC infrastructure, now and into the future, especially if we want to grow it!</li>
  <li>We have ambitious international scientific competitors — both a threat (if we can’t compete computationally) and an opportunity (for collaboration)</li>
  <li>All of which is why NERC needs a strategy …</li>
</ul>

<p>(Incidentally, the preference for lots of post-it notes in this presentation is a homage to town-hall meetings past and their flip-charts and “stick your comment on these posters” sesssions! EPSRC, I’m looking at you :-) )</p>


</div>

<div class="meeting">
<h1 class="theader"> Extreme Data Workshop</h1>
<h1 class="tdetails">Jülich, September, 2018</h1>
<h2 id="beating-data-bottlenecks-in-weather-and-climate-science">Beating Data Bottlenecks in Weather and Climate Science</h2>

<p>Presentation: <a href="/assets/talks/180918_extreme_data.pdf">pdf</a> (6 MB)</p>

<p>The data volumes produced by simulation and observation are large, and becoming larger. In the case of simulation, plans for future modelling programmes require complicated orchestration of data, and anticipate large user communities.  “Download and work at home” is no longer practical for many applications.  In the case of simulation these issues are exacerbated by users who want simulation data at grid point resolution instead of at the resolution resolved by the mathematics, and who design numerical experiments without knowledge of the storage costs.</p>

<p>There is no simple solution to these problems: user education, smarter compression, and better use of tiered storage and smarter workflows are all necessary - but far from sufficient. In this presentation we introduce two approaches to addressing (some) of these data bottlenecks: dedicated data analysis platforms, and smarter storage software.  We provide a brief introduction to the JASMIN data storage and analysis facility, and some of the storage tools and approaches being developed by the ESIWACE project.  In doing so, we describe some of our observations of real world data handling problems at scale, from the generic performance of file systems to the difficulty of optimising both volume stored and performance of workflows.   We use these examples to motivate the two-pronged approach of smarter hardware and smarter software - but recognise that data bottlenecks may yet limit the aspirations of our science.</p>

<dl class="wp-caption aligncenter" style="max-width: 600px;">

<dt><a href=""><img class="" src="/assets/images/2018-09-18-extreme-data-attendees.jpg" alt="Meeting Attendees" /></a></dt>

<dd>Meeting Attendees</dd>
</dl>

<p>(At at a <a href="http://www.fz-juelich.de/ias/jsc/EN/Expertise/Workshops/Conferences/Extreme-Data-2018/_node.html">workshop</a> organised by Martin Schultz at Jülich)</p>

</div>

<div class="meeting">
<h1 class="theader"> Open Meeting for Hydro-JULES - Next generation land-surface and hydrological predictions</h1>
<h1 class="tdetails">Wallingford, September, 2018</h1>
<h2 id="building-a-community-hydrological-model">Building a Community Hydrological Model</h2>

<p>Presentation: <a href="/assets/talks/2018-09-10-lawrence-hydrojules.pdf">pdf</a> (9 MB)</p>

<p>In this talk we introduce how NCAS will be supporting the <a href="https://www.ceh.ac.uk/our-science/projects/hydro-jules">Hydro-Jules Project</a> by designing and implmenting the modelling framework and building and maintaing and archive of driving data, model configurations, and supporting datasets. We will also be providing training and support for the community on the <a href="https://www.jasmin.ac.uk">JASMIN</a> supercomputer.</p>

<p>We also discuss some of the issue that HydroJules will need to prepare for, including the impending change to the UK Unified Modelling modelling framework and exascale computing.</p>

</div>

<div class="meeting">
<h1 class="theader"> JASMIN User Conference</h1>
<h1 class="tdetails">Milton, June, 2018</h1>
<h2 id="the-changing-nature-of-jasmin">The Changing Nature of JASMIN</h2>

<p>Presentation: <a href="/assets/talks/2018-06-27-JASMIN-UC.pdf">pdf</a> (9 MB)</p>

<p>This talk was part of a set of four to give attendees at the JASMIN user conference some understanding of the recent and planned changes to the physical JASMIN environment.</p>

<p>The introduction covers a logical and schematic view of the JASMIN system and why it exists, before three sections covering the compute usage, data movement, and storage growth over recent years. JASMIN shows nearly linear growth in total users, active users with data acess and active users of both the LOTUS batch cluster and the interactive generic science machines. Despite the changing size of the batch cluster (it has grown in size) we have managed to keep utilisation in the target 60-80% range (we have deliberately targeted a lower utilisation rate so as to allow the use of batch to be more immediate, given that keeping the data online is the more expensive part of this system). Usage of the managed cloud systems has been substantial, and the cloud itself has grown, targetting more customised solutions for a wide array of tenants. External cloud usage has been relatively low, which reflects the lack of elasticity and its usage for primarily <a href="http://cloudscaling.com/blog/cloud-computing/the-history-of-pets-vs-cattle/">pets rather than cattle</a>. Where JASMIN is really unique however is in the amount of data movement invovled in the day to day business, with PB/day being sustained in the batch cluster for significant periods. Archive growth has been capped, but shows some interesting trends, as does the volume of Sentinel data held - and overall growth has been linear despite a range of external constraints and self-limiting behaviours.  Elastic tape usage started small, but has become more signficant as disk space constraints have become more of an issue - this despite a relatively poor set of user facing features.</p>

<p>These factors (and others) led to the 2017/18 phase 4 capital upgrade which is being deployed now, with a range of new storage types. Looking forward, it is clear that the “everything on disk” is probably not the right strategy and we have to look to smarter use of tape.</p>

</div>

<div class="meeting">
<h1 class="theader"> Data-Intensive weather and climate science</h1>
<h1 class="tdetails">Exeter, June, 2018</h1>
<h2 id="climate-data-issues-systems-and-opportunities">Climate Data: Issues, Systems, and Opportunities</h2>

<p>Presentation: <a href="/assets/talks/2018-06-25-climate_data.pdf">pdf</a> (25 MB)</p>

<p>The aim of this talk was to introduce students at the Mathematics for Planet Earth summer school in Exeter to some of the issues in data science. I knew I would be following Wilco Hazeleger who was talking on Climate Science and Big Data, so I didn’t have to hit all the big data messages.</p>

<p>My aim was to cover some of the usual isues around heterogeneity and tools, but really cover some of the upcoming volume issues, using as many real numbers as I could. One of the key points I was making was that as we go forward in time, we are moving from a science dominated by the <em>difficulty</em> of <em>simulation</em> to one that will be dominated by the <em>difficulty</em> of <em>data handling</em> - and that problem is really here now, although clearly the transition to exascale will involve problems with both data and simulation. I also wanted to get across some o fthe difficulties associated with next generation model intercomparison - interdependency and data handling - and how those apply to both the modellers themselves as well as the putative users of the simulation data.</p>

<p>The 1km model future is interesting in terms of data handling. I made a slightly preposterous extrapolation (an exercise for the reader is to work out what is preposterous) … but only to show the potential scale of the problem, and the many opportunities for doing something about it.</p>

<p>The latter part of the talk covered some of the same ground as my data science talk from March, to give the students a taste of some of the things that can done with modern techniques and (often) old data.</p>

<p>The last slide was aimed at reminding folks that climate science has always been a data science, and actually, always a big data science! Climate data is always large compared to what others are actually handling … and that we have always managed to address the upcoming data challenges. I hope now will be no different.</p>

</div>

<div class="meeting">
<h1 class="theader"> EuroHPC Requirements Workshop</h1>
<h1 class="tdetails">Brussels, June, 2018</h1>
<h2 id="eurohpc-requirements-from-weather-and-climate">EuroHPC: Requirements from Weather and Climate</h2>

<p>Presentation: <a href="/assets/talks/2018-06-13-EuroHPC_Esiwace_v3.pdf">pdf</a> (9 MB)</p>

<p>Abstract: <a href="/assets/abstracts/2018-06-13-EuroHPC_ESIWACE_OnePageRequirements.pdf">pdf</a></p>

<p>This talk covered requirements for the upcoming pre-exascale and
exascale computers to be procured by the EuroHPC project. The bottom line
is that Weather and Climate have strong constraints on HPC environnments, and
we believe that procurement benchmarks should measure science throughput
in science units (in our case Simulated Years Per (real) Day, SYPD).  We
also recommend that the EuroHPC project takes cognisance of the fact that HPC
simulations do not directly generate knowledge, the knowledge comes from
analysis of the data products!</p>

</div>

<div class="meeting">
<h1 class="theader"> Data Sciences for Climate and Environment (Alan Turing Institute)</h1>
<h1 class="tdetails">London, March, 2018</h1>
<h2 id="opportunities-and-challenges-for-data-science-in-big-environmental-science">Opportunities and Challenges for Data Science in (Big) Environmental Science</h2>

<p>Presentation: <a href="/assets/talks/2018-03-26_data_science.pdf">pdf</a> (18 MB) (See also <a href="https://www.youtube.com/watch?v=sqf79JDY83A">video</a>).</p>

<p>I was asked to give a talk on data science in climate science. After working out what “data science” might mean for this audience, I took a rather larger view of
what was needed and talked about data issues in environmental science, before quickly talking about hardware and software platform issues. Most of the talk covered a few applications of modern
data science: data assimilation, classification, homogenising data, and using machine learning to infer new products. I finished by reminding everyone that in collaborations between climate
science and statisticians and computer scientists, we need to be careful about our use of the
word “model” (with a bit of help from <a href="https://xkcd.com/1838/">xkcd</a>).  I finished with reminding everyone that climate science has always been a data science.</p>

<p>The full set of videos from all the speakers is <a href="https://www.youtube.com/playlist?list=PLuD_SqLtxSdUVT_2SSPzZSC__kAxpkm8w">available</a>.</p>

<p><img src="/assets/images/2018-03-26-xkcd_machine_unlearning_2x.png" alt="Teaser Image" /></p>

</div>

<div class="meeting">
<h1 class="theader"> ESiWACE General Assembly</h1>
<h1 class="tdetails">Berlin, December, 2017</h1>
<h2 id="exploiting-weather--climate-data-at-scale-wp4">Exploiting Weather &amp; Climate Data at Scale (WP4)</h2>

<p>Presentation: <a href="/assets/talks/171212_esiwace_wp4.pdf">pdf</a> (4 MB)</p>

<p>This was a talk I would have given in partnership with Julian Kunkel, but as I was still at home thanks to a wee bit of cold air causing chaos at LHR, Julian had to give all of it. The version linked here is the version I would have given, the actual version Julian gave will (eventually) be available on the ESIWACE website.</p>

<p><img src="/assets/images/2017-12-12-esiwace_eg.png" alt="Teaser Image" /></p>

<p>The talk covers the “exploitability” component of the <a href="https://www.esiwace.eu">ESIWACE</a> project. The work we describe is development of cost models for exascale HPC storage, plus new software to write and manage data at scale.</p>

</div>

<div class="meeting">
<h1 class="theader"> European Big Data Value Forum</h1>
<h1 class="tdetails">Versailles, November, 2017</h1>
<h2 id="the-data-deluge-in-high-resolution-climate-and-weather-simulation">The Data Deluge in High Resolution Climate and Weather Simulation</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/171122_HPC_Bigdata_Joussaume_VF.pdf">pdf</a> (5 MB).</p>

<p>This talk was given by Sylvie Joussaume, but we had worked on it together, so I think it’s fair enough to include here. We wanted to show the scale of the data problems we have in climate science, and some of the direction in which we are moving with respect to “big data” technologies and algorithms.</p>

</div>

<div class="meeting">
<h1 class="theader"> Science and the Digital Revolution -  Data, Standards, and Integration</h1>
<h1 class="tdetails">Royal Society, London, November, 2017</h1>
<h2 id="data-interoperability-and-integration-a-climate-modelling-perspective">Data Interoperability and Integration: A climate modelling perspective</h2>

<p>Presentation: <a href="/assets/talks/2017-11-14-171104_data_interop_lawrence.pdf">pdf</a> (11.5 MB).</p>

<p>I was asked to give a talk at a <a href="http://www.codata.org/">CODATA</a> meeting which was aimed at developing a roadmap for:</p>
<ol>
  <li>Mobilising community support and advice for discipline-based initiatives to develop online data capacities and services;</li>
  <li>Priorities for work on interdisciplinary data integration and flagship projects;</li>
  <li>Approaches to funding and coordination; and</li>
  <li>Issues of international data governance.</li>
</ol>

<p>For this talk I was asked to address an example from the WMO research community on what we have accomplished in standardising a range of things, and reflecting on what has worked/failed and why.  I wasn’t given much time to prepare, so this is what they got!</p>

</div>

<div class="meeting">
<h1 class="theader"> Gung Ho Network Meeting</h1>
<h1 class="tdetails">Exeter University, July 2017</h1>
<h2 id="performance-portability-productivity-which-two-do-you-want">Performance, Portability, Productivity: Which two do you want?</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2017-07-18-ppp4_gungho.pdf">pdf</a> (4 MB).</p>

<p>I talked about two papers that I’ve recently been involved with: “CPMIP: measurement of real computational performance of Earth System Models in CMIP6” (which appeared in early 2017) and “Crossing the Chasm: How to develop weather
and climate models for next generation computers?” which at the time was just about to be submitted to GMD.</p>

</div>

<div class="meeting">
<h1 class="theader"> International Supercomputing (ISC) and JASMIN User Conferences</h1>
<h1 class="tdetails">Frankfurt and Didcot, June 2017</h1>
<p>I gave two versions of this talk, one at at the International Supercomputing Conference’s Workshop on HPC I/O in the data centre, and one at the 2017 JASMIN User’s Conference.</p>

<p>The talk covered the structure and usage of JASMIN, showing there is a lot of data movement both in the batch system and the interactive environment.  One key observation was that we cannot afford to carry on with parallel disk, and we don’t think tape alone is a solution, so we are investigating object stores, and
object store interfaces.</p>

<h2 id="the-uk-jasmin-environmental-commons">The UK JASMIN Environmental Commons</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2017-06-22-lawrence_isc_io.pdf">pdf</a> (12 MB - the ISC version).</p>

<h2 id="the-uk-jasmin-environmental-commons-now-and-into-the-future">The UK JASMIN Environmental Commons: Now and into the Future</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2017-06-27-lawrence_jasmin.pdf">pdf</a> (12 MB - the JASMIN user conference version).</p>

</div>

<div class="meeting">
<h1 class="theader"> NERC Town Hall Meeting on Data Centre Futures</h1>
<h1 class="tdetails">London, October, 2016</h1>
<h2 id="data-centre-technology-to-support-environmental-science">Data Centre Technology to Support Environmental Science</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2016-10-13-lawrence_infrastructure.pdf">pdf</a> (18 MB).</p>

<p>This was a talk given at a NERC Town Hall meeting on the future of data centres in London, on the 13th of October 2016.  My brief was to talk about underlying infrastructure, which I did here by discussing the relationship between scientific data workflows and the sort of things we do with JASMIN.</p>

</div>

<div class="meeting">
<h1 class="theader"> Meteorology meets Computer Science Symposium</h1>
<h1 class="tdetails">University of Reading, September 2016</h1>
<h2 id="computer-science-issues-in-environmental-infrastructure">Computer Science Issues in Environmental Infrastructure</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2016-09-15-lawrence.pdf">pdf</a> (14 MB).</p>

<p>This was a talk at a University of Reading symposium held with Tony Hey, Geoffrey Fox  and Jeff Dozier as guest speakers as part of the introduction o the new Computer Science Department in the Reading University School of Mathematical, Physical and Computer Sciences <a href="http://www.reading.ac.uk/smpcs-home.aspx">SMPCS</a>.</p>

<p>The main aim of my talk was to get across the wide range of interesting generic science and engineering challenges we face in delivering the infrastructure needed for environmental science.</p>

</div>

<div class="meeting">
<h1 class="theader"> JASMIN User Conference</h1>
<h1 class="tdetails">RAL, June, 2016</h1>
<h2 id="science-drivers-why-jasmin">Science Drivers: Why JASMIN?</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2016-06-27-jasmin_lawrence.pdf">pdf</a> (19 MB).</p>

<p>Keynote scene setter for the inaugural JASMIN user conference: how the rise of simulation leads to a data deluge and the necessity for JASMIN, and a programme to improve our data analysis techniques and infrastructure.</p>

</div>

<div class="meeting">
<h1 class="theader"> CEDA Vocbulary Meeting</h1>
<h1 class="tdetails">RAL, March, 2016</h1>
<h2 id="a-ten-minute-introduction-to-es-doc-technology">A ten minute introduction to ES-DOC technology</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2016-03-01-esdoc10minutes.pdf">pdf</a> (2 MB).</p>

<p>A brief introduction to some of the basic tools being use to define <a href="/projects/metafor">ES-DOC</a> CIM2 and the CMIP6 extensions.</p>

</div>

<div class="meeting">
<h1 class="theader"> IS-ENES2 2nd General Assembly</h1>
<h1 class="tdetails">Hamburg, February, 2016</h1>
<h2 id="esdoc-for-cmip6">ESDOC for CMIP6</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2016-02-18-esdoc4cmip6.pdf">pdf</a> (1 MB).</p>

<p>This was an introductino to how <a href="/projects/metafor">ES-DOC</a> is planning on supporting CMIP6.</p>

</div>

<div class="meeting">
<h1 class="theader"> International Computing in Atmospheric Science (ICAS)</h1>
<h1 class="tdetails">Annecy, September, 2015</h1>
<h2 id="uk-academic-infrastructure-to-support-big-environmental-science">UK academic infrastructure to support (big) environmental science</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2015-09-14-lawrence_icas15.pdf">pdf</a> (18 MB).</p>

<p>Abstract:  Modern environmental science requires the fusion of ever growing volumes of data from multiple simulation and observational platforms. In the UK we are investing in the infrastructure necessary to provide the generation, management, and analysis of the relevant datasets. This talk discusses the existing and planned hardware and software infrastructure required to support the (primarily) UK academic community in this endeavour, and relates it to key international endeavours at the European and global scale – including earth observation programmes such as the Copernicus Sentinel missions, the European Network for Earth Simulation, and the Earth System Grid Federation.</p>

</div>

<div class="meeting">
<h1 class="theader"> RCUK Cloud Workshop</h1>
<h1 class="tdetails">Warwick, June, 2015</h1>
<h2 id="why-cloud-earth-systems-science">Why Cloud? Earth Systems Science</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2015-06-04-Lawrence_Cloud.pdf">pdf</a> (6 MB).</p>

<p>Alternative title: <strong>Data Driven Science: Bringing Computation to the Data</strong>. This talk covered the background trends and described the JASMIN approach.</p>

</div>

<div class="meeting">
<h1 class="theader"> EGU</h1>
<h1 class="tdetails">Vienna, April, 2015</h1>
<h2 id="beating-the-tyranny-of-scale-with-a-private-cloud-configured-for-big-data">Beating the tyranny of scale with a private cloud configured for Big Data</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2015-04-17-Lawrence_Tyranny.pdf">pdf</a> (5 MB).</p>

<p>At the last minute I found that I wasn’t able to attend, but Phil Kershaw gave my talk. The abstract is available <a href="http://meetingorganizer.copernicus.org/EGU2015/EGU2015-12931-1.pdf">here (pdf)</a>.</p>

</div>

<div class="meeting">
<h1 class="theader"> Big Data and Extreme-Scale Computing (BDEC)</h1>
<h1 class="tdetails">Barcelona, January, 2015</h1>
<p>There were two back to back meetings organised as part of the 2015 Big Data and Extreme Computing meeting <a href="http://www.exascale.org/bdec/agenda/barcelona">website</a>.  In the first, organised as part of the European Exascale Software Initiative (EESI), I gave a full talk, in the second, I provided a four page position paper with a four page exposition.</p>

<h2 id="it-starts-and-ends-with-data-towards-exascale-from-an-earth-system-science-perspective">It starts and Ends with Data: Towards exascale from an earth system science perspective</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2015-01-28-Lawrence_EESI2015.pdf">pdf</a> (7 MB).</p>

<p>Six sections: the big picture, background trends, hardware issues, software issues, workflow, and a summary.</p>

<h2 id="bringing-compute-to-the-data">Bringing Compute to the Data</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2015-01-30-Lawrence_4slides.pdf">pdf</a> (3 MB).</p>

<p>This was my main BDEC contribution. There was also a four page
summary paper: <a href="http://0.0.0.0:4000/assets/papers/Law15.pdf">pdf</a>.</p>

</div>

<div class="meeting">
<h1 class="theader"> AGU Fall Meeting</h1>
<h1 class="tdetails">San Francisco, December, 2014</h1>
<p>I was honoured to be the third recipient of the AGU <a href="http://honors.agu.org/section-named-lecture/leptoukh-lecture-earth-and-space-science-informatics/">Leptoukh Lecture</a> awarded for significant contributions to informatics, computational, or data sciences.</p>

<h2 id="trends-in-computing-for-climate-research">Trends in Computing for Climate Research</h2>

<p>Presentation: <a href="/assets/talks/2014-12-17-lawrence_leptoukh.pdf">pdf</a> (30 MB).</p>

<p><strong>Abstract:</strong></p>

<p>The grand challenges of climate science will stress our informatics infrastructure severely in the next decade. Our drive for ever greater simulation resolution/complexity/length/repetition, coupled with new remote and in-situ sensing platforms present us with problems in computation, data handling, and information management, to name but three. These problems are compounded by the background trends: Moore’s Law is no longer doing us any favours: computing is getting harder to exploit as we have to bite the parallelism bullet, and Kryder’s Law (if it ever existed) isn’t going to help us store the data volumes we can see ahead. The variety of data, the rate it arrives, and the complexity of the tools we need and use, all strain our ability to cope. The solutions, as ever, will revolve around more and better software, but “more” and “better” will require some attention.</p>

<p>In this talk we discuss how these issues have played out in the context of CMIP5, and might be expected to play out in CMIP6 and successors. Although the CMIPs will provide the thread, we will digress into modelling per se, regional climate modelling (CORDEX), observations from space (Obs4MIPs and friends), climate services (as they might play out in Europe), and the dependency of progress on how we manage people in our institutions. It will be seen that most of the issues we discuss apply to the wider environmental sciences, if not science in general. They all have implications for the need for both sustained infrastructure and ongoing research into environmental informatics.</p>

</div>

<div class="meeting">
<h1 class="theader"> Symposium on HPC and Data-Intensive Apps</h1>
<h1 class="tdetails">Trieste, November, 2014</h1>
<p>Or to give it it’s full name: <strong>Symposium on HPC and Data-Intensive Applications in Earth Sciences: Challenges and Opportunities@ICTP, Trieste, Italy</strong>.</p>

<p>I gave two talks at this meeting, the first in the HPC regular session, on behalf of my colleague Pier Luigi Vidale, on UPSCALE, the second a data keynote on day two.</p>

<h2 id="weather-and-climate-modelling-at-the-petascale-achievements-and-perspectives-the-roadmap-to-primavera">Weather and Climate modelling at the Petascale: achievements and perspectives. The roadmap to PRIMAVERA</h2>

<p>Presentation: <a href="/assets/talks/2014-11-13-Vidale.pdf">pdf</a> (37 MB).</p>

<p><strong>Abstract:</strong></p>

<p>Recent results and plans from the Joint Met Office/NERC High Resolution Climate Modelling programme are presented, along with a summary of recent and planned model developments. We show the influence of high resolution on a number of important atmospheric phenomena, highlighting both the roles of multiple groups in the work and the need for further resolution and complexity improvements in multiple models. We introduce plans for a project to do just that. A final point is that this work is highly demanding of both the supercomputing and subsequent analysis environments.</p>

<h2 id="infrastructure-for-environmental-supercomputing-beyond-the-hpc">Infrastructure for Environmental Supercomputing: beyond the HPC!</h2>

<p><strong>Abstract:</strong></p>

<p>We begin by motivating the problems facing us in environmental simulations across scales: complex community interactions, and complex infrastructure. Looking forward we see the drive to increased resolution and complexity leading not only to compute issues, but even more severe data storage and handling issues. We worry about the software consequences before moving to the only possible solution, more and better collaboration, with shared infrastructure. To make progress requires moving past consideration of software interfaces alone to consider also the “collaboration” interfaces. We spend considerable time describing the JASMIN HPC data collaboration environment in the UK, before reaching the final conclusion: Getting our models to run on (new) supercomputers is hard. Getting them to run perfomantly is hard. Analysing, exploiting and archiving the data is (probably) now even harder!</p>

<p>Presentation:  <a href="/assets/talks/2014-11-14-Lawrence_ICTP.pdf">pdf</a> (22 MB )</p>

</div>

<div class="meeting">
<h1 class="theader"> NERC ICT Current Awareness</h1>
<h1 class="tdetails">Warwick, October, 2014</h1>
<h2 id="jasmin---a-nerc-data-analysis-environment">JASMIN - A NERC Data Analysis Environment</h2>

<p>Presentation: <a href="/assets/talks/2014-10-07-Lawrence_JASMIN.pdf">pdf</a> (18MB).</p>

<p>The talk basically covered an explanation of what JASMIN actually consists of, and provides, and it’s relationship to the Cloud.  It included some discussion of why JASMIN exists in the context of the growing data problem in the community.</p>

</div>

<div class="meeting">
<h1 class="theader"> NCAS Science Meeting</h1>
<h1 class="tdetails">Bristol, July, 2014</h1>
<h2 id="the-influence-of-moores-law-and-friends-on-our-computing-environment">The influence of Moore’s Law and friends on our computing environment!</h2>

<p>Presentation: <a href="/assets/talks/2014-07-17-Lawrence_NCAS_Sci.pdf">pdf</a> (19 MB).</p>

<p>I gave a talk on how Moore’s Law and friends are influencing atmospheric science, the infrastructure we need, and how we trying to deliver services to the community.</p>

</div>

<div class="meeting">
<h1 class="theader"> e-Research NZ</h1>
<h1 class="tdetails">Hamilton, June/July, 2014</h1>
<p>I gave three talks at this meeting:</p>

<h2 id="environmental-modelling-at-both-large-and-small-scales-how-simulating-complexity-leads-to-a-range-of-computing-challenges">Environmental Modelling at both large and small scales: How simulating complexity leads to a range of computing challenges</h2>

<p>Presentation: <a href="/assets/talks/2014-06-30-lawrence_eresearchNZ14_hpc.pdf">pdf</a> (2 MB).</p>

<p>On Monday, in the HPC workshop, despite using the same title I had for the Auckland seminar, I primarily talked about the importance of software supporting collaboration, using coupling as the exemplar (reprising some of the material I presented in Boulder in early 2013).</p>

<h2 id="the-road-to-exascale-for-climate-science-crossing-borders-or-crossing-disciplines-can-one-do-both-at-the-same-time">The road to exascale for climate science: crossing borders or crossing disciplines, can one do both at the same time?</h2>

<p>Presentation: <a href="/assets/talks/2014-07-01-lawrence_eresearchNZ14_keynote.pdf">pdf</a> (9 MB)</p>

<p>On Tuesday I gave the keynote address:</p>

<p><em>Abstract:</em> The grand challenges of climate science have significant infrastructural implications, which lead to requirements for integrated e-infrastructure - integrated at national and international scales, but serving users from a variety of disciplines. We begin by introducing the challenges, then discuss the implications for computing, data, networks, software, and people, beginning from existing activities, and looking out as far as we can see (spoiler alert: not far!)</p>

<h2 id="jasmin-the-joint-analysis-system-for-big-data">JASMIN: the Joint Analysis System for big data</h2>

<p><a href="/assets/talks/2014-07-02-lawrence_eresearchNZ14_jasmin.pdf">pdf</a> (5 MB)</p>

<p>On Wednesday I gave a short talk on JASMIN:</p>

<p><em>Abstract:</em> JASMIN is designed to deliver a shared data infrastructure for the UK environmental science community.  We describe the hybrid batch/cloud environment and some of the compromises we have made to provide a curated archive inside and alongside various levels of managed and unmanaged cloud … touching on the difference between backup and archive at scale. Some examples of JASMIN usage are provided, and the speed up on workflows we have achieved. JASMIN has just recently been upgraded, having originally been designed for atmospheric and earth observation science, but now being required to support a wider community. We discuss what was upgraded, and why.</p>

</div>

<div class="meeting">
<h1 class="theader"> IS-ENES2 Kickoff Meeting</h1>
<h1 class="tdetails">Paris, France, May, 2013</h1>
<h2 id="the-future-of-esgf-in-the-context-of-enes-and-is-enes2">The Future of ESGF in the context of ENES and IS-ENES2</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2013-05-28-lawrence-isenes2-kickoff.pdf">pdf</a> (2MB).</p>

<p>I probably tried to do too much in this talk. There were three subtexts:</p>
<ol>
  <li>We as a community have too much data to handle, and I mentioned the apocryphal estimate that only 2/3 of data written is read … but I confused folks … that figure applies to institutional data, not data in ESGF …</li>
  <li>That the migration of data and information between domains (see the talk) requires a lot of effort, and that (nearly) no one recognises or funds that effort (kudos to KNMI :-),</li>
  <li>That portals are easy to build, but hard to build right, and maybe we need fewer, or maybe we need more, but either way, they need to <em>both</em> meet requirements in technical functionality, and information (as opposed to data) content.</li>
</ol>

</div>

<div class="meeting">
<h1 class="theader"> Coupling Workshop (CW2013)</h1>
<h1 class="tdetails">Boulder, Colorado, February, 2013</h1>
<h2 id="bridging-communities-technical-concerns-for-building-integrated-environmental-models">Bridging Communities: Technical Concerns for Building Integrated Environmental Models</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2013-02-21-lawrence-cw2013.pdf">pdf</a> (1MB).</p>

</div>

<div class="meeting">
<h1 class="theader"> 2nd IS-ENES Workshop on High-performance computing for Climate Models</h1>
<h1 class="tdetails">Toulouse, France, January, 2013</h1>
<h2 id="data-the-elephant-in-the-room-jasmin-one-step-along-the-road-to-dealing-with-the-elephant">Data, the elephant in the room. JASMIN one step along the road to dealing with the elephant.</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2013-01-28-lawrence-hpc-enes.pdf">pdf</a> (2MB).</p>

</div>

<div class="meeting">
<h1 class="theader"> AGU Fall meeting</h1>
<h1 class="tdetails">San Francisco, December, 2012</h1>
<h2 id="issues-to-address-before-we-can-have-an-open-climate-modelling-ecosystem">Issues to address before we can have an open climate modelling ecosystem</h2>

<p>Presentation: <a href="/assets/talks/2012-12-06-LawrenceAGU2012.pdf">pdf</a> (2 MB).</p>

<p><em>Authors</em>: Lawrence, Balaji, DeLuca, Guilyardi, Taylor</p>

<p><em>Abstract</em> Earth system and climate models are complex assemblages of code which are
an optimisation of what is known about the real world, and what we can afford
to simulate of that knowledge. Modellers are generally experts in one part of
the earth system, or in modelling itself, but very few are experts across the
piste.  As a consequence, developing and using models (and their output)
requires expert teams which in most cases are the holders of the “institutional
wisdom” about their model, what it does well,and what it doesn’t.  Many of
us have an aspiration for an open modelling ecosystem, not only to provide
transparency and provenance for results, but also to expedite the modelling
itself. However an open modelling ecosystem will depend on opening access to
code, to inputs, to outputs, and most of all, on opening the access to that
institutional wisdom (in such a way that the holders of such wisdom are
protected from providing undue support for third parties). Here we present
some of the lessons learned from how the metafor and curator projects
(continuing forward as the es-doc consortium) have attempted to encode such wisdom as documentation. We will concentrate on both technical and social issues that we have uncovered, including a discussion of the place of peer review and
citation in this ecosystem.</p>

<p>(This is a modified version of the abstract submitted to AGU, to more fairly reflect the content given the necessity to cut material to fit into the 15 minute slot available.)</p>

</div>

<div class="meeting">
<h1 class="theader"> ESA CCI 3rd Colocation Meeting</h1>
<h1 class="tdetails">Frascati, September, 2012</h1>
<h2 id="exploiting-high-volume-simulations-and-observations-of-the-climate">Exploiting high volume simulations and observations of the climate</h2>

<p>Presentation: <a href="/assets/talks/2012-09-24-lawrence_esa_cci_v04.pdf">pdf</a> (7 MB).</p>

<p>An introduction to ENES and ESGF with some scientific motivation.</p>


</div>

<div class="meeting">
<h1 class="theader"> ICT Competitiveness</h1>
<h1 class="tdetails">Trieste, September, 2012</h1>
<h2 id="weather-and-climate-computing-futures-in-the-context-of-european-competitiveness">Weather and Climate Computing Futures in the context of European Competitiveness</h2>

<p>Presentation: <a href="/assets/talks/2012-09-19-Lawrence_ICTBigData.pdf">pdf</a> (4 MB).</p>

<p>In this talk I addressed some elements of how climate science interacts with policy and societal competitiveness in the contentxt of extreme climate events etc, but the main body was on the consequences for modelling and underlying infrastructure.</p>

<p>This table drives much of the conversation:</p>

<table>
  <thead>
    <tr>
      <th>Key numbers for Climate Earth System Modelling</th>
      <th style="text-align: right">2012</th>
      <th style="text-align: right">2016</th>
      <th style="text-align: right">2020</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Horizontal resolution of each coupled model component (km)</td>
      <td style="text-align: right">125</td>
      <td style="text-align: right">50</td>
      <td style="text-align: right">10</td>
    </tr>
    <tr>
      <td>Increase in horizontal parallelisation wrt 2012 <br />(hyp: weak scaling in 2 directions)</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">6.25</td>
      <td style="text-align: right">156.25</td>
    </tr>
    <tr>
      <td>Horizontal parallelization of each coupled model component<br />(number of cores)</td>
      <td style="text-align: right">1,00E+03</td>
      <td style="text-align: right">6,25E+03</td>
      <td style="text-align: right">1,56E+05</td>
    </tr>
    <tr>
      <td>Vertical resolution of each coupled model component<br />(number of levels)</td>
      <td style="text-align: right">30</td>
      <td style="text-align: right">50</td>
      <td style="text-align: right">100</td>
    </tr>
    <tr>
      <td>Vertical parallelization of each coupled model component</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">10</td>
    </tr>
    <tr>
      <td>Number of components in the coupled model</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">5</td>
    </tr>
    <tr>
      <td>Number of members in the ensemble simulation</td>
      <td style="text-align: right">10</td>
      <td style="text-align: right">20</td>
      <td style="text-align: right">50</td>
    </tr>
    <tr>
      <td>Number of models/groups in the ensemble experiments</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">4</td>
    </tr>
    <tr>
      <td>Total number of cores (4x6x7x8x9)<br /> (Increase:)</td>
      <td style="text-align: right">8,00E+04<br />(1)</td>
      <td style="text-align: right">1,00E+06<br />(13)</td>
      <td style="text-align: right">1,56E+09<br />(19531)</td>
    </tr>
    <tr>
      <td>Data produced (for one component in<br /> Gbytes/month-of-simulation)</td>
      <td style="text-align: right">2,5</td>
      <td style="text-align: right">26</td>
      <td style="text-align: right">1302</td>
    </tr>
    <tr>
      <td>Data produced in total (in<br /> Gbytes/month-of-simulation)</td>
      <td style="text-align: right">200</td>
      <td style="text-align: right">4167</td>
      <td style="text-align: right">1302083</td>
    </tr>
    <tr>
      <td>Increase</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">21</td>
      <td style="text-align: right">6510</td>
    </tr>
  </tbody>
</table>

<p>The bottom line in the talk was that to deliver on all of this requires
European scale infrastructure, that is, computing AND networks targeted at
data analysis as well as data production!</p>

</div>

<div class="meeting">
<h1 class="theader"> An Aussie Triumvirate</h1>
<h1 class="tdetails">Canberra and Gold Coast, November, 2010</h1>
<p>These were three talks given in Australia during a short trip in November, 2010. I gave one in Canberra, and two on the Gold Coast.</p>

<h2 id="british-experience-with-building-standards-based-networks-for-climate-and-environmental-research">British experience with building standards based networks for climate and environmental research</h2>

<p>Presentation: <a href="/assets/talks/2010-11-05-networking.pdf">pdf</a> (5 MB)</p>

<p>This was the keynote talk for the <em>Information Network Workshop, Canberra, November, 2010</em>.</p>

<p>The talk covered organisational and technological drivers to network interworking, with some experience from the UK and European context, and some comments for the future.</p>

<p>All the other talks from the meeting are available on the <a href="http://www.osdm.gov.au/Events/271.aspx">osdm website</a>.</p>

<h2 id="rethinking-metadata-to-realise-the-full-potential-of-linked-scientific-data">Rethinking metadata to realise the full potential of linked scientific data</h2>

<p>Presentation: <a href="/assets/talks/2010-11-08-metadata.pdf">pdf</a> (3 MB)</p>

<p><em>Metadata Workshop, Gold Coast, November 2010</em></p>

<p>This talk begins with an introduction to our metafor taxonomy, and why metadata, and metadata tooling, are important. There is an extensive discussion of the importance of model driven architectures, and plans for extending our existing formalism to support both RDF and XML serialisations. We consider our the observations and measurements paradigm needs extension to support climate science, and discuss quality control annotation.</p>

<p>All the talks from this meeting are available on an
[http://ands.org.au/events/metadataworkshop08-11-2010/index.html ANDS website].</p>

<h2 id="provenance-metadata-and-e-infrastructure-to-support-climate-science">Provenance, metadata and e-infrastructure to support climate science</h2>

<p>Presentation: <a href="/assets/talks/2010-11-11-eResearch-keynote.pdf">pdf</a> (9 MB)</p>

<p>This was a keynote for the <a href="https://ocs.arcs.org.au/index.php/eRAust/2010/schedConf/program">Australian e-Research Conference, 2010</a>.</p>

<p><em>Abstract:</em> The importance of data in shaping our day to day decisions is understood by the person on the street. Less obviously, metadata is important to our decision making: how up to date is my account
balance? How does the cost of my broadband supply compare with the offer I just read in the newspaper? We just don’t think of those things as metadata (one persons data is another persons
metadata). Similarly, the importance of data in shaping our future decisions about reacting to climate change is obvious. Less obvious, but just as important, is the provenance of the data:who produced it/them, how, using what technique, is the difficulty of the interpretation in anyway consistent with the skills of the interpreter? In this talk I’ll introduce some key parts of the metadata spectrum underlying our efforts to document climate data, for use now and into the future. In particular, we’ll discuss the information modelling and metadata pipeline being
constructed to support the currently active global climate model inter-comparison project known as CMIP5. In doing so we’ll touch on the metadata activities of the European Metafor project, the
software developments being sponsored by the US Earth System Grid and European IS-ENES projects, and how all these activities are being integrated into a global federated e-infrastructure.</p>

<p>All conference talks are available  <a href="https://ocs.arcs.org.au/index.php/eraust/2010/schedConf/presentations all conference talks">here</a>.</p>


</div>

<div class="meeting">
<h1 class="theader"> NSF Cyberinfrastructure for Data</h1>
<h1 class="tdetails">Redmond, USA, September, 2010</h1>
<h2 id="cyberinfrastructure-challenges-from-a-climate-science-repository-perspective">Cyberinfrastructure Challenges (from a climate science repository perspective)</h2>

<p>Presentation: <a href="/assets/talks/2010-09-30-cyberinfrastructure_for_data">pdf</a> (2MB).</p>

<p>I gave a very short presentation at this NSF sponsored workshop.
See <a href="/computing/2010/09/cyberinfrastructure_for_data/">my blog entry</a>
for a commentary.</p>

<p><strong>Update</strong> (<em>Bryan, 31st January, 2017</em>): The output of this workshop eventually appeared in an <a href="https://www.nsf.gov/cise/oac/taskforces/TaskForceReport_Data.pdf">NSF Report</a>.</p>

</div>

<div class="meeting">
<h1 class="theader"> ENES Earth System Modelling Scoping Meeting</h1>
<h1 class="tdetails">Montvillargennes, March, 2010</h1>
<h2 id="software--data-infrastructure-for-earth-system-modelling">Software &amp; Data Infrastructure for Earth System Modelling</h2>

<p>Presentation: <a href="/assets/talks/2010-03-30-european-esm-strategy.pdf">pdf</a> (1 MB).</p>

<p>This meeting was targeted as being the first step in a foresight process for establishing a European earth system modelling strategy. This is the talk on software and data infrastructure prepared for the meeting (authored with Eric Guilyardi and Sophie Valcke).</p>

</div>

<div class="meeting">
<h1 class="theader"> RDF and Ontology Workshop</h1>
<h1 class="tdetails">Edinburgh, June, 2006</h1>
<h2 id="distributed-data-distributed-governance-distributed-vocabularies-the-nerc-datagrid">Distributed Data, Distributed Governance, Distributed Vocabularies: The NERC DataGrid</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2006-06-08-NDG_SemanticEdinburgh06.ppt">pdf</a> ( 6 MB).</p>

<p>The workshop page is available <a href="http://wiki.esi.ac.uk/RDF,_Ontologies_and_Meta-Data_Workshop">here</a>!</p>

</div>

<div class="meeting">
<h1 class="theader"> NERC e-Science All-Hands-Meeting</h1>
<h1 class="tdetails">AHM, April, 2006</h1>
<h2 id="nerc-datagrid-status">NERC DataGrid Status</h2>

<p>Presentation: <a href="http://0.0.0.0:4000/assets/talks/2006-04-26-nerc-escience-ahm.ppt">pdf</a> (5 MB).</p>

<p>In this presentation, I present some of the motivation for the <a href="http://ndg.nerc.ac.uk">NERC DataGrid</a> development (the key points being that we want semantic access to distributed data with no centralised user management), link it to the ISO TC211 standards work, and take the listener through a tour of some of the NDG products as they are now. There is a slightly more detailed look at the Climate Sciences Modelling Language, and I conclude with an overview of the NDG roadmap.</p>

</div>

</div>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">
    <hr/>
<!--     <h2 class="footer-heading">Bryan Lawrence</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li><strong>Bryan Lawrence</strong></li>
		  <li>Professor of Weather and Climate Computing</li>
          <li><a href="mailto:b.n.lawrence@reading.ac.uk">b.n.lawrence@reading.ac.uk</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">

          
          <li>
              <a href="https://github.com/bnlawrence">
              <i class="fa fa-github" style="color:gray"></i> bnlawrence
             </a>
          </li>
          

          
          <li>
             <a href="https://twitter.com/bnlawrence">
             <i class="fa fa-twitter" style="color:gray"></i> bnlawrence
             </a>
          </li>
          

        </ul>
      </div>

      <div class="footer-col  footer-col-3">
         <p class="text">
	     This is a personal site. Caveat Lector. Nothing written here reflects an official opinion of my employer or any funding agency. Laboris locatores analogum obsecro respice auferetur. (My daughter is learning Latin, unlike me, who never did!)

      </div>
    </div>
    <!--
    <hr/>
    <p class="text"> This is a personal site. Caveat Lector. Nothing written here reflects an official opinion of my employer or any funding agency. Laboris locatores analogum obsecro respice auferetur. (My daughter is learning Latin, unlike me, who never did!)
 </p>
    -->

  </div>

</footer>

    
  </body>


</html>
<!-- d.050600.062508.030515.080516 | "Baby, I'm Yours" -->
