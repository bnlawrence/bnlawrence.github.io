<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bryan Lawrence</title>
    <description>Intermittent rambings on climate, computing, or data science, what I'm up to, and what I'm thinking. Mostly professional, sometimes not.
</description>
    <link>https://www.bnlawrence.net//</link>
    <atom:link href="https://www.bnlawrence.net//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 03 Dec 2017 08:20:41 +0000</pubDate>
    <lastBuildDate>Sun, 03 Dec 2017 08:20:41 +0000</lastBuildDate>
    <generator>Jekyll v3.6.2</generator>
    
      <item>
        <title>Dr Matt Jones</title>
        <description>&lt;p&gt;A good day for any academic - when one of her or his students passes their viva (oral examination) for their doctorate.&lt;/p&gt;

&lt;p&gt;Today, Matt Jones defended his thesis: &lt;em&gt;Parallel Data Analysis for Atmospheric Science&lt;/em&gt;. Congrats Matt!&lt;/p&gt;

&lt;h2 id=&quot;matts-abstract&quot;&gt;Matt’s Abstract&lt;/h2&gt;

&lt;p&gt;Data sizes are growing in atmospheric science, as climate models increase to higher resolutions
to improve the representation of atmospheric phenomena, and larger numbers of ensemble
members are used as to better capture the variability in the atmosphere. New methods
need to be developed to handle the increasing size of data – traditional analysis scripts often
inefficiently read and process data, leading to excessive analysis times. Research into large
data analysis often focuses on providing solutions in the form of software, or hardware, rather
than providing quantitative results on what factors can reduce performance in an application.
This thesis quantitatively investigates these factors in the software-hardware stack, in order
to make decisions of how to handle the large data sizes during application development and
data management. This is done in the context of an atmospheric science workflow in a high performance
computing environment.&lt;/p&gt;

&lt;p&gt;A major bottleneck in analysis in atmospheric science is reading data. Two of the primary
factors which are commonly known to affect the read rate are the read pattern, and the read
size. These factors are found in this work to reduce the read rate by up to 10-50 times for poor
combinations. Other factors which could affect the read rate for atmospheric analysis include:
the programming language, the libraries used, and the file layout.&lt;/p&gt;

&lt;p&gt;NetCDF4 is one of the most commonly used data formats in atmospheric science, and the
Python library netCDF4-python is one of the main interfaces used. As part of the NetCDF4
file format, there are options for chunking (multidimensional tiling), and inbuilt compression,
which can be used to improve read and write performance from the files. It was found that
at peak performance the netCDF4-python library performs 40% worse than the underlying C
NetCDF4 library. With respect to chunking and compression, poor combinations of chunking,
and inbuilt compression, were found to reduce the performance by over 100 times.&lt;/p&gt;

&lt;p&gt;One solution to reduced performance, or a way to reduce analysis times on large datasets,
is to run applications in parallel. It is important to understand how, on a particular platform,
application relevant parallel reads will scale in order design an efficient application. The parallel
scaling of the JASMIN super-data cluster was analysed. The investigation methodology,
and conclusions from the investigation can be applied to other platforms.
A case study was used to apply the results from this work in a real atmospheric science
workflow – a space-time spectral analysis technique. It confirmed that these results do indeed
apply to real workflows.&lt;/p&gt;
</description>
        <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
        <link>https://www.bnlawrence.net//academic/2017/11/congrats2matt/</link>
        <guid isPermaLink="true">https://www.bnlawrence.net//academic/2017/11/congrats2matt/</guid>
        
        <category>teaching</category>
        
        <category>hpc_io</category>
        
        
        <category>academic</category>
        
      </item>
    
      <item>
        <title>European Big Data Value Forum</title>
        <description>&lt;h2 id=&quot;the-data-deluge-in-high-resolution-climate-and-weather-simulation&quot;&gt;The Data Deluge in High Resolution Climate and Weather Simulation&lt;/h2&gt;

&lt;p&gt;Presentation: &lt;a href=&quot;https://www.bnlawrence.net//assets/talks/171122_HPC_Bigdata_Joussaume_VF.pdf&quot;&gt;pdf&lt;/a&gt; (5 MB).&lt;/p&gt;

&lt;p&gt;This talk was given by Sylvie Joussaume, but we had worked on it together, so I think it’s fair enough to include here. We wanted to show the scale of the data problems we have in climate science, and some of the direction in which we are moving with respect to “big data” technologies and algorithms.&lt;/p&gt;
</description>
        <pubDate>Wed, 22 Nov 2017 00:00:00 +0000</pubDate>
        <link>https://www.bnlawrence.net//talks/2017/11/HPC_Bigdata/</link>
        <guid isPermaLink="true">https://www.bnlawrence.net//talks/2017/11/HPC_Bigdata/</guid>
        
        
        <category>talks</category>
        
      </item>
    
      <item>
        <title>Science and the Digital Revolution - Data, Standards, and Integration</title>
        <description>&lt;p&gt;I was asked to give a talk at this &lt;a href=&quot;http://www.codata.org/&quot;&gt;CODATA&lt;/a&gt; meeting which was aimed at developing a roadmap for:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mobilising community support and advice for discipline-based initiatives to develop online data capacities and services;&lt;/li&gt;
  &lt;li&gt;Priorities for work on interdisciplinary data integration and flagship projects;&lt;/li&gt;
  &lt;li&gt;Approaches to funding and coordination; and&lt;/li&gt;
  &lt;li&gt;Issues of international data governance.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Details of my talk are &lt;a href=&quot;/talks/2017/11/data_interop_lawrence/&quot;&gt;here&lt;/a&gt;.
&lt;!-- More --&gt;
There were some other interesting talks, some of which resonated particularly with me, including&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.dataone.org/organization/executive-team/william-bill-michener&quot;&gt;Bill Michener&lt;/a&gt; talking about Dataone, and the importance of governance activities where all lines of responsibility are clear: no committees or panels whose remit is unclear or whose reporting lines are non-existent.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.southampton.ac.uk/chemistry/about/staff/jgf.page&quot;&gt;Jeremy Fry&lt;/a&gt; talking about digital infrastructure in support of (physical) chemistry, who had some thought provoking comments on creativity, “engrooved” bad habits (harking back to a talk by Paul Trowler, which I have yet to get a copy of) and the potential role of AI in scientific discovery:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt; Also, human creativity seems to depend increasingly on the stochasticity of previous experiences - particular life events that allow a researcher to notice something others do not. Although chance has always been a factor in scientific discovery, it is currently playing a much larger role than it should. *([The Atlantic, April 2017](https://www.theatlantic.com/science/archive/2017/04/can-scientific-discovery-be-automated/524136/))*.
* [Robert Hanisch](https://www.nist.gov/people/robert-hanisch)  talking about experience of the IAU in developing technical infrastructure, and in particular the governance of technical components for the International Virtual Observatory Alliance. The  [IVOA](http://www.ivoa.net/) attempts to follows the W3C governance model:
    * Data access protocols require two independent implementations.
    * Notes are promoted to Working Drafts, which become Proposed Recommendations, which get endorsed to become Recommendations, which eventually become standards &amp;#151; although the did note that the formal endorsement step wasn't (yet?) in practice.
&lt;/blockquote&gt;

&lt;p&gt;All communities reported difficulties in funding infrastructure at the sorts of levels necessary, and little or no investment in usability … funders like prototypes … but not boring maintenance and improvement … which means that sometimes scientific take-up falls far below what was envisaged.&lt;/p&gt;

&lt;p&gt;Unfortunately I was not able to go to days one and three of this three day meeting.&lt;/p&gt;
</description>
        <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
        <link>https://www.bnlawrence.net//computing/2017/11/science_and_the_digital_revolution/</link>
        <guid isPermaLink="true">https://www.bnlawrence.net//computing/2017/11/science_and_the_digital_revolution/</guid>
        
        
        <category>computing</category>
        
      </item>
    
      <item>
        <title>Science and the Digital Revolution -  Data, Standards, and Integration</title>
        <description>&lt;h2 id=&quot;data-interoperability-and-integration-a-climate-modelling-perspective&quot;&gt;Data Interoperability and Integration: A climate modelling perspective&lt;/h2&gt;

&lt;p&gt;Presentation: &lt;a href=&quot;https://www.bnlawrence.net//assets/talks/2017-11-14-171104_data_interop_lawrence.pdf&quot;&gt;pdf&lt;/a&gt; (11.5 MB).&lt;/p&gt;

&lt;p&gt;I was asked to give a talk at a &lt;a href=&quot;http://www.codata.org/&quot;&gt;CODATA&lt;/a&gt; meeting which was aimed at developing a roadmap for:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Mobilising community support and advice for discipline-based initiatives to develop online data capacities and services;&lt;/li&gt;
  &lt;li&gt;Priorities for work on interdisciplinary data integration and flagship projects;&lt;/li&gt;
  &lt;li&gt;Approaches to funding and coordination; and&lt;/li&gt;
  &lt;li&gt;Issues of international data governance.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For this talk I was asked to address an example from the WMO research community on what we have accomplished in standardising a range of things, and reflecting on what has worked/failed and why.  I wasn’t given much time to prepare, so this is what they got!&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Nov 2017 00:00:00 +0000</pubDate>
        <link>https://www.bnlawrence.net//talks/2017/11/data_interop_lawrence/</link>
        <guid isPermaLink="true">https://www.bnlawrence.net//talks/2017/11/data_interop_lawrence/</guid>
        
        
        <category>talks</category>
        
      </item>
    
      <item>
        <title>Gung Ho Network Meeting</title>
        <description>&lt;h2 id=&quot;performance-portability-productivity-which-two-do-you-want&quot;&gt;Performance, Portability, Productivity: Which two do you want?&lt;/h2&gt;

&lt;p&gt;Presentation: &lt;a href=&quot;https://www.bnlawrence.net//assets/talks/2017-07-18-ppp4_gungho.pdf&quot;&gt;pdf&lt;/a&gt; (4 MB).&lt;/p&gt;

&lt;p&gt;I talked about two papers that I’ve recently been involved with: “CPMIP: measurement of real computational performance of Earth System Models in CMIP6” (which appeared in early 2017) and “Crossing the Chasm: How to develop weather
and climate models for next generation computers?” which at the time was just about to be submitted to GMD.&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Jul 2017 00:00:00 +0100</pubDate>
        <link>https://www.bnlawrence.net//talks/2017/07/ppp4_gungho/</link>
        <guid isPermaLink="true">https://www.bnlawrence.net//talks/2017/07/ppp4_gungho/</guid>
        
        
        <category>talks</category>
        
      </item>
    
      <item>
        <title>International Supercomputing (ISC) and JASMIN User Conferences</title>
        <description>&lt;p&gt;I gave two versions of this talk, one at at the International Supercomputing Conference’s Workshop on HPC I/O in the data centre, and one at the 2017 JASMIN User’s Conference.&lt;/p&gt;

&lt;p&gt;The talk covered the structure and usage of JASMIN, showing there is a lot of data movement both in the batch system and the interactive environment.  One key observation was that we cannot afford to carry on with parallel disk, and we don’t think tape alone is a solution, so we are investigating object stores, and
object store interfaces.&lt;/p&gt;

&lt;h2 id=&quot;the-uk-jasmin-environmental-commons&quot;&gt;The UK JASMIN Environmental Commons&lt;/h2&gt;

&lt;p&gt;Presentation: &lt;a href=&quot;https://www.bnlawrence.net//assets/talks/2017-06-22-lawrence_isc_io.pdf&quot;&gt;pdf&lt;/a&gt; (12 MB - the ISC version).&lt;/p&gt;

&lt;h2 id=&quot;the-uk-jasmin-environmental-commons-now-and-into-the-future&quot;&gt;The UK JASMIN Environmental Commons: Now and into the Future&lt;/h2&gt;

&lt;p&gt;Presentation: &lt;a href=&quot;https://www.bnlawrence.net//assets/talks/2017-06-27-lawrence_jasmin.pdf&quot;&gt;pdf&lt;/a&gt; (12 MB - the JASMIN user conference version).&lt;/p&gt;
</description>
        <pubDate>Tue, 27 Jun 2017 00:00:00 +0100</pubDate>
        <link>https://www.bnlawrence.net//talks/2017/06/lawrence_jasmin/</link>
        <guid isPermaLink="true">https://www.bnlawrence.net//talks/2017/06/lawrence_jasmin/</guid>
        
        
        <category>talks</category>
        
      </item>
    
      <item>
        <title>Week Eight</title>
        <description>&lt;p&gt;Spent nearly the entire week (and certainly more than a “European maximum 48-hours”) on things to do with ESIWACE (on the deliverable  I mentioned last time, reporting for the EC, etc). I did have a fair chunk of one day on other things as also chaired a meeting of the advisory panel for our climate predictions for the copernicus climate data store project  (CP4CDS, a project about deploying and maintaining sofware for a special ESGF data node to support climate services) - that took up the best part of day. However, that’s pretty much it for the week. Hard to believe we’re two months into the year - and I still haven’t come up for air and managed to create any significant blogging time.&lt;/p&gt;
</description>
        <pubDate>Mon, 27 Feb 2017 00:00:00 +0000</pubDate>
        <link>https://www.bnlawrence.net//diary/2017/02/week_eight/</link>
        <guid isPermaLink="true">https://www.bnlawrence.net//diary/2017/02/week_eight/</guid>
        
        
        <category>diary</category>
        
      </item>
    
      <item>
        <title>Weeks Six and Seven</title>
        <description>&lt;p&gt;Two weeks to report this time, primarily because I had a day off on sick leave, and two days on real leave, so there’s only seven weekday workdays to talk about (although
I have just spent bit chunks of both yesterday and today, that is, sat and sun, on work as well).&lt;/p&gt;

&lt;p&gt;Early on the main thing I was doing was trying to catch up on the ever
increasing email mountain, but  in week six, by the time I took out a day in
London for a NERC Information Strategy Group meeting (mostly about the future of
NERC data centres), a half day on technical futures for CEDA, another half day
on my final CEDA monthly meeting and a follow up meeting on CEDA support for
&lt;a href=&quot;http://www.sparc-climate.org/&quot;&gt;SPARC&lt;/a&gt;, and a day off on sick leave, that only
left a few hours here and there to get much done. The net effect of course was
that the email mountain grew.&lt;/p&gt;

&lt;p&gt;To be fair, it wasn’t so much that the email mountain grew, but the Nozbe task
list grew. I did manage to process a lot of email, but quite a lot of things
got thrown onto the pile for “later”.&lt;/p&gt;

&lt;p&gt;Then this last week (week seven), we had a few days on holiday down Dorset/Devon way.
Just the couple of nights, but real recharge territory. However, back to work
on Wednesday, and back to London - this time for an NCAS Science Strategy
Board meeting, so only two real work days available.&lt;/p&gt;

&lt;p&gt;So, in the work time, I did a wee bit of work on the ENES infrastructure foresight that I’ve talked about before, and quite a lot of work on the first deliverable we’ve
got for &lt;a href=&quot;http://www.esiwace.eu&quot;&gt;ESIWACE&lt;/a&gt;: which is on requirements and business
modelling for (weather and climate) data centres.  Given the constraints on my
time during the week, it’s been a big effort on that this weekend too.  I suspect
quite a few things from that will turn up on my blog …&lt;/p&gt;

&lt;p&gt;… but anyway, that’s the weeks that were.&lt;/p&gt;
</description>
        <pubDate>Sun, 19 Feb 2017 00:00:00 +0000</pubDate>
        <link>https://www.bnlawrence.net//diary/2017/02/weeks_six_and_seven/</link>
        <guid isPermaLink="true">https://www.bnlawrence.net//diary/2017/02/weeks_six_and_seven/</guid>
        
        
        <category>diary</category>
        
      </item>
    
      <item>
        <title>Space and Open Plan Offices</title>
        <description>&lt;p&gt;I was paying a bit more attention to twitter this morning than usual (I’m hoping I’ll  get some feedback on my analysis of citations that I posted yesterday). One thing that blew by was this headline &lt;a href=&quot;https://www.washingtonpost.com/posteverything/wp/2014/12/30/google-got-it-wrong-the-open-office-trend-is-destroying-the-workplace/?tid=ss_tw&amp;amp;utm_term=.e08d2e093c59&quot;&gt;the Washington Post&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&quot;Google got it wrong. The open-office trend is destroying the workplace.&quot;
&lt;/blockquote&gt;

&lt;p&gt;which took me back to &lt;a href=&quot;/management/2005/03/peopleware/&quot;&gt;something that I wrote in 2005&lt;/a&gt;, reporting on work done long before.&lt;/p&gt;

&lt;p&gt;It seems that Google was ignoring history as well. Unlike them.&lt;/p&gt;

&lt;p&gt;There’s been lots more work done since the work I cited in that blog post, &lt;a href=&quot;https://www.dezeen.com/2016/09/15/open-plan-offices-co-working-less-productive-more-unfriendly-survey-auckland-university-technology/&quot;&gt;for example&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;Workers in open-plan offices are more distracted, unfriendly and uncollaborative than those in traditional workplaces, according to the latest industry survey.
&lt;/blockquote&gt;
&lt;blockquote&gt;Employees who have to share their office with more than two people experience high levels of colleague distrust and form fewer co-worker friendships than those working in single-occupancy offices ...
&lt;/blockquote&gt;

&lt;p&gt;or &lt;a href=&quot;https://digest.bps.org.uk/2013/08/19/the-supposed-benefits-of-open-plan-offices-do-not-outweigh-the-costs/&quot;&gt;this&lt;/a&gt; reporting &lt;a href=&quot;http://dx.doi.org/10.1016/j.jenvp.2013.06.007&quot;&gt;published research&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&quot;...the open-plan proponents' argument that open-plan improves morale and productivity appears to have no basis in the research literature.&quot;&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 06 Feb 2017 00:00:00 +0000</pubDate>
        <link>https://www.bnlawrence.net//management/2017/02/space_and_open_plan_offices/</link>
        <guid isPermaLink="true">https://www.bnlawrence.net//management/2017/02/space_and_open_plan_offices/</guid>
        
        <category>management</category>
        
        
        <category>management</category>
        
      </item>
    
      <item>
        <title>Week Five</title>
        <description>&lt;p&gt;Nothing exciting to report. I spent nearly the entire week processing email, interacting with my team (both directly and on slack), and producing short things (e.g. a management level one pager on why NCAS should continue to support the &lt;a href=&quot;http://cfconventions.org&quot;&gt;CF conventions&lt;/a&gt;, and how). However, I did spend a wee bit of time trying to reinforce some of my observations around why storage is becoming a much bigger deal in environmental science. More on that next …&lt;/p&gt;
</description>
        <pubDate>Sun, 05 Feb 2017 00:00:00 +0000</pubDate>
        <link>https://www.bnlawrence.net//diary/2017/02/week_five/</link>
        <guid isPermaLink="true">https://www.bnlawrence.net//diary/2017/02/week_five/</guid>
        
        
        <category>diary</category>
        
      </item>
    
  </channel>
</rss>
