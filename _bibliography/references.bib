
@article{LawChasm17,
  timestamp = {2017-12-05T14:30:04Z},
  title = {Crossing the {{Chasm}}: {{How}} to Develop Weather and Climate Models for next Generation Computers.},
  doi = {10.5194/gmd-2017-186},
  journal = {Geoscientific Model Development Discussions},
  author = {Lawrence, Bryan N. and {Mike Rezny} and {Reinhard Budich} and {Peter Bauer} and {J{\"o}rg Behrens} and {Mick Carter} and {Willem Deconinck} and {Rupert Ford} and {Christopher Maynard} and {Steve Mullerworth} and {Carlos Osuna} and {Andy Porter} and {Kim Serradell} and {Sophie Valcke} and {Nils Wedi} and {Simon Wilson}},
  keywords = {bnl,onblog,prp17}
}

@inproceedings{SmarEA17,
  timestamp = {2017-12-05T13:54:59Z},
  title = {A {{Scalable Object Store}} for {{Meteorological}} and {{Climate Data}}},
  isbn = {978-1-4503-5062-4},
  doi = {10.1145/3093172.3093238},
  language = {en},
  urldate = {2017-11-27},
  publisher = {{ACM Press}},
  author = {Smart, Simon D. and Quintino, Tiago and Raoult, Baudouin},
  year = {2017},
  keywords = {object stores,onblog,storage},
  pages = {1--8},
  file = {Smart et al. - 2017 - A Scalable Object Store for Meteorological and Cli.pdf:/Users/BNL28/zotero-storage/storage/JAQMUWPT/Smart et al. - 2017 - A Scalable Object Store for Meteorological and Cli.pdf:application/pdf;PASC17 presentation.pdf:/Users/BNL28/zotero-storage/storage/MFQ7JBPT/PASC17 presentation.pdf:application/pdf}
}

@article{AciEA15,
  timestamp = {2017-12-05T14:29:51Z},
  title = {Architectures and Methodologies for Future Deployment of Multi-Site {{Zettabyte}}-{{Exascale}} Data Handling Platforms},
  volume = {664},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/664/4/042009},
  abstract = {Several scientific fields, including Astrophysics, Astroparticle Physics, Cosmology, Nuclear and Particle Physics, and Research with Photons, are estimating that by the 2020 decade they will require data handling systems with data volumes approaching the Zettabyte distributed amongst as many as 10 18 individually addressable data objects (Zettabyte-Exascale systems). It may be convenient or necessary to deploy such systems using multiple physical sites. This paper describes the findings of a working group composed of experts from several},
  language = {en},
  number = {4},
  urldate = {2017-12-03},
  journal = {Journal of Physics: Conference Series},
  author = {Ac{\'\i}n, V. and Bird, I. and Boccali, T. and Cancio, G. and Collier, I. P. and Corney, D. and Delaunay, B. and Delfino, M. and {dell'Agnello}, L. and Flix, J. and {P Fuhrmann} and Gasthuber, M. and G{\"u}lzow, V. and Heiss, A. and Lamanna, G. and Macchi, P.-E. and Maggi, M. and Matthews, B. and Neissner, C. and Nief, J.-Y. and Porto, M. C. and Sansum, A. and Schulz, M. and Shiers, J.},
  year = {2015},
  keywords = {onblog,storage},
  pages = {042009},
  annote = {Extracted Annotations (03/12/2017, 20:46:03)"introduction of a global Storage Virtualization Layer which is logically separated from the individual storage sites" (Ac{\'\i}n et al 2015:42011)"for maximal simplification and automation in the deployment of the physical sites;" (Ac{\'\i}n et al 2015:42011)"the need to present the user with an integrated view of their custom metadata and technical metadata (such as the last time an object was accessed, etc.);" (Ac{\'\i}n et al 2015:42011)"Although computing and data processing technology is known for its fast growth, assembling these technologies into large-scale, reliable and performant systems often takes about a decade. Hence, it is important to start work towards the Zettabyte-Exascale." (Ac{\'\i}n et al 2015:42012)"The computational and data processing loads generated by these teams are enormous compared to other digital tasks, even those described as "Big Data"." (Ac{\'\i}n et al 2015:42012)"The challenge is often compounded by the fact that the exact manner in which the data will need to be analysed is not known a priori, hence techniques often used in commercial computing, such as pre-calculation, precise data placement or massive data caching are difficult or impossible to implement." (Ac{\'\i}n et al 2015:42012)"Increased importance of scientific metadata, with an increasing fraction of analysis activities using metadata as its primary input. \textbullet{} The need to handle technical metadata, such as object creation or last-access time, in an integrated manner with respect the scientific metadata. This should also include support for data provenance and data preservation requirements. \textbullet{} Linked-data aspects, where support is needed to "zoom" from metadata to other forms of data, including the raw data of experiments. \textbullet{} The overall increase in scale. The increase in number of objects will affect data and metadata equally, which will need Exascale support. The increase in data volume indicates that when raw data approaches the Zettabyte scale, the metadata handling system will approach the Petabyte scale. This increased volume is implicitly accompanied by an increase in data throughput requirements, which are often difficult for users to fully specify" (Ac{\'\i}n et al 2015:42013)"The number of users involved in a given scientific project is probably the only parameter which is not growing exponentially. Current solutions for managing users and their data access roles are, however, very far from being satisfactory. Present systems involve a lot of manual intervention by many parties, and therefore are rather error prone and cumbersome. Furthermore, they are poorly (or not at all) integrated with low level data processing services and between different sites." (Ac{\'\i}n et al 2015:42014)"File systems offer virtually no practical support for associating user-specified metadata to data objects, essentially limited to specifying file names. Although essentially all scientific projects encode some metadata in file names, this is not enough to fulfil their requirements and they maintain in parallel an ad hoc metadata management scheme as part of their data management setups. Maintaining these schemes is labour intensive, and would become even more so if support was included for data provenance and preservation." (Ac{\'\i}n et al 2015:42014)"It should be noted that object storage is not particularly new to scientific projects using magnetic tape for their storage, since tape has rarely offered file system support and storage on tape has been historically handled by storing objects on tape and maintaining the metadata separately." (Ac{\'\i}n et al 2015:42014)"Currently, there are many data virtualization schemes which are deployed as part of large distributed file systems, such as dCache [12], EOS [13], GPFS [14] and Lustre [15]. Deployments are restricted to single sites, with a few exceptions [16], and only manage the technical metadata needed to emulate a traditional file system. In parallel, redirector schemes have been deployed which return handles (in the form of URLs) to file system based storage at multiple sites. Examples are the xrootd and http/WebDAV redirectors deployed over the last few years by the CERN LHC experiments." (Ac{\'\i}n et al 2015:42015)"There is, however, a widening gap between the possibilities offered by these leading-edge networks and the capabilities that are actually deployed in production applications. Features such as dynamic bandwidth provisioning or traffic prioritization are seldom deployed in production." (Ac{\'\i}n et al 2015:42015)},
  file = {IOP Full Text PDF:/Users/BNL28/zotero-storage/storage/M7S3946Q/Ac√≠n et al. - 2015 - Architectures and methodologies for future deploym.pdf:application/pdf}
}

@techreport{ASAC2014,
  timestamp = {2017-12-05T14:17:38Z},
  title = {Top Ten Exascale Research Challenges},
  author = {{ASCAC Subcommittee}},
  collaborator = {Lucas, Robert},
  month = feb,
  year = {2014},
  keywords = {onblog},
  file = {Subcommittee - 2014 - Top ten exascale research challenges.pdf:/Users/BNL28/zotero-storage/storage/RC5MMEL8/Subcommittee - 2014 - Top ten exascale research challenges.pdf:application/pdf}
}

@techreport{DonEA14,
  timestamp = {2017-12-05T14:18:31Z},
  title = {Applied Mathematics Research for Exascale Computing},
  institution = {{Lawrence Livermore National Laboratory (LLNL), Livermore, CA}},
  author = {Dongarra, Jack and Hittinger, Jeffrey and Bell, John and Chacon, Luis and Falgout, Robert and Heroux, Michael and Hovland, Paul and Ng, Esmond and Webster, Clayton and Wild, Stefan},
  year = {2014},
  keywords = {onblog},
  file = {EMWGreport.pdf:/Users/BNL28/zotero-storage/storage/XGZ2JTLR/EMWGreport.pdf:application/pdf}
}

@misc{NextPOSIX,
  timestamp = {2017-12-05T14:14:25Z},
  title = {What's {{So Bad About POSIX I}}/{{O}}?},
  abstract = {POSIX I/O is almost universally agreed to be one of the most significant limitations standing in the way of I/O performance exascale system designs push 10},
  urldate = {2017-12-05},
  howpublished = {https://www.nextplatform.com/2017/09/11/whats-bad-posix-io/},
  journal = {The Next Platform},
  author = {Lockwood, Glen},
  month = sep,
  year = {2017},
  keywords = {onblog},
  file = {Snapshot:/Users/BNL28/zotero-storage/storage/YEF4JESD/whats-bad-posix-io.html:text/html}
}


